{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akashlevy/RRAM-DNN-Quantized/blob/main/resnet18_cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIMyvw8HUQTS"
      },
      "source": [
        "# **Setup: Imports & Helper Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7Y_YxSGUjvB"
      },
      "source": [
        "Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZqJSXnnUM65"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.quantization\n",
        "from torch.quantization import QuantStub, DeQuantStub\n",
        "from torch.quantization import *\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import pandas as pd\n",
        "import statistics\n",
        "import scipy.stats\n",
        "from scipy.sparse import diags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SIzEeeTVvfM"
      },
      "source": [
        "Loading Testing and Training Data from CIFAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161,
          "referenced_widgets": [
            "2858e963ea25467ca961b8c6f3a6590d",
            "994ddccb10fb450dbd89222b6be68725",
            "21df95b1fb464b7d86014e6def00259a",
            "f7c874ead2c84024a3dfb7adb996b041",
            "4d45d791982642318ba62a0e66b24ba2",
            "1b7a94d8e1ab4dc19fe39f67fd355ce9",
            "d8075a4f16ce40e38ff62c42126e0f35",
            "e78df50b3d024f588da3466ab457dc98",
            "93b769b0ee8546a5bbaca76de642fbe6",
            "c1a9d58ffd054fbaa299d39dde4b114d",
            "0c4e2778cbd24b21b38ee37fc94ac805"
          ]
        },
        "id": "du56m3E1Vy0u",
        "outputId": "f4654fc4-b97a-4da9-b6d0-f1b3f103531e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2858e963ea25467ca961b8c6f3a6590d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Pad(4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32),\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root = './data',train=True,download=True,transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset,batch_size=64,shuffle=True,num_workers=16,pin_memory=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True,transform=transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64,shuffle=False,num_workers=16,pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_C4AHv3XjE5"
      },
      "source": [
        "Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myE6N8sDXkWK"
      },
      "outputs": [],
      "source": [
        "class AverageMeter(object):\n",
        "    #computes and stores average and current value, used in training\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "def accuracy(output, target):\n",
        "    #computes top-1 accuracy\n",
        "    with torch.no_grad():\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(1, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        correct_one = correct[:1].view(-1).float().sum(0, keepdim=True)\n",
        "        return correct_one.mul_(100.0 / batch_size).item()\n",
        "\n",
        "def print_size_of_model(model):\n",
        "    #print size of model\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
        "    os.remove('temp.p')\n",
        "\n",
        "def load_model(quantized_model, model):\n",
        "    #loads model into object meant for quantization\n",
        "    state_dict = model.state_dict()\n",
        "    model = model.to('cpu')\n",
        "    quantized_model.load_state_dict(state_dict)\n",
        "\n",
        "def fuse_modules(model):\n",
        "    #fuse together convolutions and other layers\n",
        "    fused_model = model\n",
        "    fused_model = torch.quantization.fuse_modules(\n",
        "    fused_model, [[\"conv1\", \"bn1\", \"relu\"]], inplace=True)\n",
        "    for module_name, module in fused_model.named_children():\n",
        "        if \"layer\" in module_name:\n",
        "            for basic_block_name, basic_block in module.named_children():\n",
        "                torch.quantization.fuse_modules(\n",
        "                    basic_block, [[\"conv1\", \"bn1\", \"relu\"], [\"conv2\", \"bn2\"]], inplace=True)\n",
        "                for sub_block_name, sub_block in basic_block.named_children():\n",
        "                    if sub_block_name == \"downsample\":\n",
        "                        torch.quantization.fuse_modules(\n",
        "                            sub_block, [[\"0\", \"1\"]], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwAeuhrFYcm9"
      },
      "source": [
        "Training and Testing Funcs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvQZDfMTYedT"
      },
      "outputs": [],
      "source": [
        "#local variable to plot loss to see convergence\n",
        "train_losses = []\n",
        "train_losses.clear()\n",
        "\n",
        "def train(model: nn.Module, dataloader: DataLoader, num_epochs, cuda=False, q=False):\n",
        "    train_losses.clear()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(),lr=0.05, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "    #scheduler impacts loss convergence for QAT vs baseline\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[25,40], gamma=0.1)\n",
        "    \n",
        "    if q == True:\n",
        "      scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[2,5,7,10,15], gamma=0.1)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        running_loss = AverageMeter('loss')\n",
        "        acc = AverageMeter('train_acc')\n",
        "        for i, data in enumerate(dataloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "            if cuda:\n",
        "              inputs = inputs.cuda()\n",
        "              labels = labels.cuda()\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if epoch>=3 and q:\n",
        "              model.apply(torch.quantization.disable_observer)\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss.update(loss.item(), outputs.shape[0])\n",
        "            acc.update(accuracy(outputs, labels), outputs.shape[0])\n",
        "            if i % 100 == 0:    # print every 100 mini-batches\n",
        "                print('[%d, %5d] ' %\n",
        "                    (epoch + 1, i + 1), running_loss, acc)\n",
        "        train_loss = running_loss.avg / len(trainloader)\n",
        "        train_losses.append(train_loss)\n",
        "        scheduler.step()\n",
        "    print('Finished Training')\n",
        "\n",
        "\n",
        "def test(model: nn.Module, dataloader: DataLoader, cuda=False) -> float:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            inputs, labels = data\n",
        "\n",
        "            if cuda:\n",
        "              inputs = inputs.cuda()\n",
        "              labels = labels.cuda()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    return 100 * correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9MqAQz-3EQ3"
      },
      "source": [
        "# **Train Unquantized ResNet-18 Model on CIFAR10**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XOAG2nCemzV"
      },
      "source": [
        "Train baseline (modified) ResNet18 on CIFAR-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aulWwSmsYrWF"
      },
      "outputs": [],
      "source": [
        "#get unquantized model from torch.models.quantization (already has built-in fusing capability)\n",
        "unquant_model = models.quantization.resnet18(pretrained=False, num_classes=10)\n",
        "\n",
        "#change the initial conv/maxpool layers so that they work better for CIFAR-10 dataset\n",
        "unquant_model.conv1 = torch.nn.Conv2d(3,64,kernel_size=3,stride=1,padding=1,bias=False)\n",
        "unquant_model.maxpool = torch.nn.Identity()\n",
        "\n",
        "#train\n",
        "unquant_model = unquant_model.cuda()\n",
        "train(unquant_model,trainloader,50,cuda=True,q=False)\n",
        "\n",
        "#plot loss\n",
        "%matplotlib inline\n",
        "plt.plot(train_losses)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64AU9Rm9Uhap"
      },
      "source": [
        "Compute baseline inference accuracy and save model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2dspZs6emGE",
        "outputId": "c0529d81-0355-460a-8712-bca305d0a8d6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the test images: 93.57% - FP32\n"
          ]
        }
      ],
      "source": [
        "unquant_model = unquant_model.cuda()\n",
        "\n",
        "score = test(unquant_model, testloader, cuda=True)\n",
        "print('Accuracy of the network on the test images: {}% - FP32'.format(score))\n",
        "\n",
        "torch.save(unquant_model.state_dict(), 'resnet18_cifar10_baseline.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRuAyyakYlJV"
      },
      "source": [
        "# **Quantization Aware Training: 8 bits**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzatZLAxVPqH"
      },
      "source": [
        "Initialize model and fuse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMQDgKVHHUNM",
        "outputId": "a1210d65-a70d-4036-d09b-fce9ad768e8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "#initialize model with same modifications as baseline\n",
        "qnet = models.quantization.resnet18(pretrained=False, num_classes=10)\n",
        "qnet.conv1 = torch.nn.Conv2d(3,64,kernel_size=3,stride=1,padding=1,bias=False)\n",
        "qnet.maxpool = torch.nn.Identity()\n",
        "\n",
        "qnet.load_state_dict(torch.load('resnet18_cifar10_baseline.pth'))\n",
        "\n",
        "#fuse model\n",
        "qnet.fuse_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2ZTBTYzVuCb"
      },
      "source": [
        "Prepare for QAT, and train neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8eczsqhMI-v"
      },
      "outputs": [],
      "source": [
        "print(qnet)\n",
        "\n",
        "#set for QAT\n",
        "qnet.eval()\n",
        "qnet.qconfig = torch.quantization.get_default_qat_qconfig()\n",
        "\n",
        "#prepare network for QAT\n",
        "qnet.train()\n",
        "torch.quantization.prepare_qat(qnet,inplace=True)\n",
        "qnet = qnet.cuda()\n",
        "\n",
        "#Train NN\n",
        "train(qnet,trainloader,20,cuda=True,q=True)\n",
        "print(\"Size of Model after quantization\")\n",
        "print_size_of_model(qnet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkwySRTkVwhm"
      },
      "source": [
        "Plot loss to confirm convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "XBucFHoyEd60",
        "outputId": "3c62b8d3-659f-4fe7-fa02-6eb539da898c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "plt.plot(train_losses)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOE7uHAmeVO2"
      },
      "source": [
        "Convert quantized model to int8 representation and compute inference accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPrtKmhTqDeJ"
      },
      "outputs": [],
      "source": [
        "qnet= qnet.cpu()\n",
        "torch.quantization.convert(qnet,inplace=True)\n",
        "print(\"Size of model after quantization\")\n",
        "print_size_of_model(qnet)\n",
        "\n",
        "#calculate inference accuracy\n",
        "score = test(qnet,testloader,cuda=False)\n",
        "print('Accuracy of the fused and quantized network (trained quantized) on the test images: {}% - INT8'.format(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqfPD0ZDV8Cp"
      },
      "source": [
        "Save model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXJgn9SvJ_IN"
      },
      "outputs": [],
      "source": [
        "torch.save(qnet.state_dict(), 'resnet18_cifar10_8b_qat.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njEiOj__eeIC"
      },
      "source": [
        "Load saved model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsw9JxyU5WKa"
      },
      "outputs": [],
      "source": [
        "qnet_plt = models.quantization.resnet18(pretrained=False, num_classes=10)\n",
        "qnet_plt.conv1 = torch.nn.Conv2d(3,64,kernel_size=3,stride=1,padding=1,bias=False)\n",
        "qnet_plt.maxpool = torch.nn.Identity()\n",
        "\n",
        "qnet_plt.train()\n",
        "qnet_plt.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
        "qnet_plt.fuse_model()\n",
        "torch.quantization.prepare_qat(qnet_plt,inplace=True)\n",
        "qnet_plt = torch.quantization.convert(qnet_plt, inplace=True)\n",
        "\n",
        "qnet_plt.load_state_dict(torch.load('resnet18_cifar10_8b_qat.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_vDF7PLuebS"
      },
      "outputs": [],
      "source": [
        "score = test(qnet_plt, testloader, cuda=False)\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGMR336JEp_w"
      },
      "source": [
        "# **Perturb weights using Confusion Matrix Error Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation: Loading model and extracting weights"
      ],
      "metadata": {
        "id": "SdHwhFNy-znV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deIyjRVGEZsz"
      },
      "source": [
        "Load weights from saved model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAFDB2XOPcr_",
        "outputId": "8846c641-87c1-4b20-9380-c8f67668b0ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:178: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  reduce_range will be deprecated in a future release of PyTorch.\"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/utils.py:281: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
            "  \"must run observer before calling calculate_qparams. \" +\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "model = models.quantization.resnet18(pretrained=False, num_classes=10)\n",
        "model.conv1 = torch.nn.Conv2d(3,64,kernel_size=3,stride=1,padding=1,bias=False)\n",
        "model.maxpool = torch.nn.Identity()\n",
        "\n",
        "model.train()\n",
        "model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
        "model.fuse_model()\n",
        "torch.quantization.prepare_qat(model,inplace=True)\n",
        "model = torch.quantization.convert(model, inplace=True)\n",
        "\n",
        "model.load_state_dict(torch.load('resnet18_cifar10_8b_qat.pth'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBWvI05tFtz_"
      },
      "source": [
        "Extract weights from model. (NOTE: need recursive method bc state_dict.items() doesn't get all layers) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights = {}\n",
        "\n",
        "def get_weights(model: nn.Module, key, curr_layer):\n",
        "  for name, child in model.named_children():\n",
        "    if isinstance(child,nn.Identity) or isinstance(child, nn.AvgPool2d) or isinstance(child, nn.quantized.QFunctional) or isinstance(child,nn.AdaptiveAvgPool2d) or isinstance(child, nn.quantized.Quantize) or isinstance(child, nn.quantized.DeQuantize):\n",
        "      continue\n",
        "    elif isinstance(child,nn.Sequential) or isinstance(child, torchvision.models.quantization.resnet.QuantizableBasicBlock):\n",
        "      if 'layer' in name:\n",
        "        curr_layer = name\n",
        "      get_weights(child, name, curr_layer)\n",
        "    else:\n",
        "      newname = curr_layer + '.' + key + '.' + name\n",
        "      weights[newname] = child.weight().int_repr().flatten().numpy()\n",
        "\n",
        "#get weights of model\n",
        "get_weights(model, \"\", \"\")\n",
        "print(weights.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmuKiPRO5IHD",
        "outputId": "cfc24f74-a559-4aa1-d6a4-3768e781b686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['..conv1', 'layer1.0.conv1', 'layer1.0.conv2', 'layer1.1.conv1', 'layer1.1.conv2', 'layer2.0.conv1', 'layer2.0.conv2', 'layer2.downsample.0', 'layer2.1.conv1', 'layer2.1.conv2', 'layer3.0.conv1', 'layer3.0.conv2', 'layer3.downsample.0', 'layer3.1.conv1', 'layer3.1.conv2', 'layer4.0.conv1', 'layer4.0.conv2', 'layer4.downsample.0', 'layer4.1.conv1', 'layer4.1.conv2', 'layer4..fc'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perturb weights using confusion matrix error model."
      ],
      "metadata": {
        "id": "A3LhfwKq64He"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regular weight perturbation.**"
      ],
      "metadata": {
        "id": "JMR3OHd583Yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "depositivize = np.vectorize(lambda x : x - 1 if x > 127 else x)\n",
        "\n",
        "for seed in range(3,4):\n",
        "  np.random.seed(seed)\n",
        "      \n",
        "  #sweep BERs\n",
        "  for ber in 10.**np.linspace(-8,0,50):\n",
        "    #log\n",
        "    print(f'Doing {seed}\\t{ber}')\n",
        "\n",
        "    nrows = 4\n",
        "    ncols = 4\n",
        "    \n",
        "    #create confmat\n",
        "    offset = [-1, 0, 1]\n",
        "    k = [[ber]*(ncols-1), [1 - 2*ber]*ncols, [ber]*(ncols-1)]\n",
        "\n",
        "    #sparse matrix with diagonals given by k\n",
        "    confmat = diags(k,offset).toarray()\n",
        "    confmat[0][0] = 1-ber\n",
        "    confmat[nrows-1][ncols-1] = 1-ber\n",
        "    \n",
        "    C_intermed = np.kron(confmat, confmat)\n",
        "    C = np.kron(C_intermed, C_intermed)\n",
        "    \n",
        "    print(len(C))\n",
        "    print(len(C[0]))\n",
        "\n",
        "    weights_perturb = {}\n",
        "\n",
        "    #perturb weights under confmat error model\n",
        "    for layer_name in weights:\n",
        "      if 'conv' in layer_name:\n",
        "        weight_in_layer = weights[layer_name]\n",
        "\n",
        "        #perturb based on confusion matrix\n",
        "\n",
        "        weights_perturb[layer_name] = np.zeros_like(weights[layer_name])\n",
        "        for x in range(256):\n",
        "          perturb = np.random.choice(256, p=C[x], size=len(weights[layer_name]))\n",
        "          weights_perturb[layer_name] += np.where(weights[layer_name] == (x-127), 1, 0) * perturb\n",
        "\n",
        "        #scale back to weight value\n",
        "        weights_perturb[layer_name] = depositivize(weights_perturb[layer_name] - 127)\n",
        "\n",
        "    #load relaxed weights back into model\n",
        "    for layer_name in weights_perturb:\n",
        "      name_arr = layer_name.split('.')\n",
        "      if name_arr[1] is \" \" or name_arr[1] is \"\":\n",
        "        name_arr[1] = \"\"\n",
        "      else:\n",
        "        name_arr[1] = \"[\" + name_arr[1] + \"].\"\n",
        "\n",
        "      if layer_name == \"..conv1\":\n",
        "        layer_name_p = \"conv1.weight\"\n",
        "      else:\n",
        "        layer_name_p = layer_name + \".weight\"\n",
        "\n",
        "      #keep scales/zero points\n",
        "      tmp = model.state_dict()[layer_name_p]\n",
        "      scales = tmp.q_per_channel_scales()\n",
        "      zero_pts = tmp.q_per_channel_zero_points()\n",
        "      axis = tmp.q_per_channel_axis()\n",
        "\n",
        "      new_w = torch._make_per_channel_quantized_tensor(torch.from_numpy(weights_perturb[layer_name]), scales, zero_pts, axis)\n",
        "        \n",
        "      setattr(model, str(str(name_arr[0] + name_arr[1] + name_arr[2]) + \"weight\"), new_w)\n",
        "\n",
        "    #evaluate accuracy after relaxation\n",
        "    # model = model.cuda()\n",
        "    accuracy = test(model,testloader,cuda=False)\n",
        "\n",
        "    with open('char_simple_kron_4x4_sweep_seed_4.tsv', 'a') as outf:\n",
        "      print(f'{seed}\\t{ber}\\t{accuracy}\\n')\n",
        "      outf.write(f'{seed}\\t{ber}\\t{accuracy}\\n')"
      ],
      "metadata": {
        "id": "s3Z5Hw0H7O1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalized weights with respect to weight frequency.**"
      ],
      "metadata": {
        "id": "FAxKpMeq7K7Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bC_OAE9E6q_g"
      },
      "outputs": [],
      "source": [
        "depositivize = np.vectorize(lambda x : x - 1 if x > 127 else x)\n",
        "\n",
        "for seed in range(1):\n",
        "  np.random.seed(seed)\n",
        "  \n",
        "  #sweep indices\n",
        "  for i in range(0,256):\n",
        "    for j in range(0,256):\n",
        "      if i == j:\n",
        "        continue\n",
        "      \n",
        "      #sweep BERs\n",
        "      ber = 1e-2\n",
        "\n",
        "      print(f'Doing {seed}\\t{i}\\t{j}')\n",
        "\n",
        "      #create confusion matrix\n",
        "      weights_perturb = {}\n",
        "\n",
        "      #perturb weights under confmat error model\n",
        "      for layer_name in weights:\n",
        "        if 'conv' in layer_name:\n",
        "          weight_in_layer = weights[layer_name]\n",
        "\n",
        "          #get unique weights and frequencies.\n",
        "          unique_vals, frequencies = np.unique(weight_in_layer, return_counts=True)\n",
        "\n",
        "          #divide ber by frequency of that particular weight\n",
        "          ber = ber / (frequencies[0][j-128]) \n",
        "          \n",
        "          #create confmat with new ber\n",
        "          C = np.eye(256)\n",
        "          C[i][i] = 1-ber\n",
        "          C[i][j] = ber\n",
        "\n",
        "          #perturb\n",
        "          weights_perturb[layer_name] = np.zeros_like(weights[layer_name])\n",
        "          for x in range(256):\n",
        "            perturb = np.random.choice(256, p=C[x], size=len(weights[layer_name]))\n",
        "            weights_perturb[layer_name] += np.where(weights[layer_name] == (x-127), 1, 0) * perturb\n",
        "\n",
        "          #scale back to weight value\n",
        "          weights_perturb[layer_name] = depositivize(weights_perturb[layer_name] - 127)\n",
        "\n",
        "      #load relaxed weights back into model\n",
        "      for layer_name in weights_perturb:\n",
        "        name_arr = layer_name.split('.')\n",
        "        if name_arr[1] is \" \" or name_arr[1] is \"\":\n",
        "          name_arr[1] = \"\"\n",
        "        else:\n",
        "          name_arr[1] = \"[\" + name_arr[1] + \"].\"\n",
        "\n",
        "        if layer_name == \"..conv1\":\n",
        "          layer_name_p = \"conv1.weight\"\n",
        "        else:\n",
        "          layer_name_p = layer_name + \".weight\"\n",
        "\n",
        "        #keep scales/zero points\n",
        "        tmp = model.state_dict()[layer_name_p]\n",
        "        scales = tmp.q_per_channel_scales()\n",
        "        zero_pts = tmp.q_per_channel_zero_points()\n",
        "        axis = tmp.q_per_channel_axis()\n",
        "\n",
        "        new_w = torch._make_per_channel_quantized_tensor(torch.from_numpy(weights_perturb[layer_name]), scales, zero_pts, axis)\n",
        "        \n",
        "        setattr(model, str(str(name_arr[0] + name_arr[1] + name_arr[2]) + \"weight\"), new_w)\n",
        "\n",
        "      #evaluate accuracy after relaxation\n",
        "      accuracy = test(model,testloader,cuda=False)\n",
        "\n",
        "      with open('char_normc_weight_freq.tsv', 'a') as outf:\n",
        "        print(f'{seed}\\t{i}\\t{j}\\t{ber}\\t{accuracy}\\n')\n",
        "        outf.write(f'{seed}\\t{i}\\t{j}\\t{ber}\\t{accuracy}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Weight perturbation but normalized c wrt PDF approximation**"
      ],
      "metadata": {
        "id": "dLfFXKw-87RJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "depositivize = np.vectorize(lambda x : x - 1 if x > 127 else x)\n",
        "\n",
        "for seed in range(1):\n",
        "  np.random.seed(seed)\n",
        "  \n",
        "  #sweep indices\n",
        "  for i in range(0,256):\n",
        "    for j in range(0,256):\n",
        "      if i == j:\n",
        "        continue\n",
        "      \n",
        "      ber = 1e-2\n",
        "\n",
        "      print(f'Doing {seed}\\t{i}\\t{j}\\t{ber}')\n",
        "\n",
        "      #create confusion matrix\n",
        "\n",
        "      #divide ber by prob given by pdf approximation\n",
        "      ber = ber / (scipy.stats.norm(0,32).pdf(j))\n",
        "\n",
        "      #create confmat with new ber    \n",
        "      C = np.eye(256)\n",
        "      C[i][i] = 1-ber\n",
        "      C[i][j] = ber\n",
        "\n",
        "      weights_perturb = {}\n",
        "\n",
        "      #perturb weights under confmat error model\n",
        "      for layer_name in weights:\n",
        "        if 'conv' in layer_name:\n",
        "          weight_in_layer = weights[layer_name]\n",
        "\n",
        "          #perturb based on confusion matrix\n",
        "\n",
        "          weights_perturb[layer_name] = np.zeros_like(weights[layer_name])\n",
        "          for x in range(256):\n",
        "            perturb = np.random.choice(256, p=C[x], size=len(weights[layer_name]))\n",
        "            weights_perturb[layer_name] += np.where(weights[layer_name] == (x-127), 1, 0) * perturb\n",
        "\n",
        "          #scale back to weight value\n",
        "          weights_perturb[layer_name] = depositivize(weights_perturb[layer_name] - 127)\n",
        "\n",
        "      #load relaxed weights back into model\n",
        "      for layer_name in weights_perturb:\n",
        "        name_arr = layer_name.split('.')\n",
        "        if name_arr[1] is \" \" or name_arr[1] is \"\":\n",
        "          name_arr[1] = \"\"\n",
        "        else:\n",
        "          name_arr[1] = \"[\" + name_arr[1] + \"].\"\n",
        "\n",
        "        if layer_name == \"..conv1\":\n",
        "          layer_name_p = \"conv1.weight\"\n",
        "        else:\n",
        "          layer_name_p = layer_name + \".weight\"\n",
        "\n",
        "        tmp = model.state_dict()[layer_name_p]\n",
        "        scales = tmp.q_per_channel_scales()\n",
        "        zero_pts = tmp.q_per_channel_zero_points()\n",
        "        axis = tmp.q_per_channel_axis()\n",
        "\n",
        "        new_w = torch._make_per_channel_quantized_tensor(torch.from_numpy(weights_perturb[layer_name]), scales, zero_pts, axis)\n",
        "        \n",
        "        setattr(model, str(str(name_arr[0] + name_arr[1] + name_arr[2]) + \"weight\"), new_w)\n",
        "\n",
        "      #evaluate accuracy after relaxation\n",
        "      accuracy = test(model,testloader,cuda=False)\n",
        "\n",
        "      with open('char_normc_pdf_approx.tsv', 'a') as outf:\n",
        "        print(f'{seed}\\t{i}\\t{j}\\t{ber}\\t{accuracy}\\n')\n",
        "        outf.write(f'{seed}\\t{i}\\t{j}\\t{ber}\\t{accuracy}\\n')"
      ],
      "metadata": {
        "id": "Vj1nprSItUxw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "resnet18_cifar10.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2858e963ea25467ca961b8c6f3a6590d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_994ddccb10fb450dbd89222b6be68725",
              "IPY_MODEL_21df95b1fb464b7d86014e6def00259a",
              "IPY_MODEL_f7c874ead2c84024a3dfb7adb996b041"
            ],
            "layout": "IPY_MODEL_4d45d791982642318ba62a0e66b24ba2"
          }
        },
        "994ddccb10fb450dbd89222b6be68725": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b7a94d8e1ab4dc19fe39f67fd355ce9",
            "placeholder": "",
            "style": "IPY_MODEL_d8075a4f16ce40e38ff62c42126e0f35",
            "value": "100%"
          }
        },
        "21df95b1fb464b7d86014e6def00259a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e78df50b3d024f588da3466ab457dc98",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_93b769b0ee8546a5bbaca76de642fbe6",
            "value": 170498071
          }
        },
        "f7c874ead2c84024a3dfb7adb996b041": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1a9d58ffd054fbaa299d39dde4b114d",
            "placeholder": "",
            "style": "IPY_MODEL_0c4e2778cbd24b21b38ee37fc94ac805",
            "value": " 170498071/170498071 [00:10&lt;00:00, 17682575.60it/s]"
          }
        },
        "4d45d791982642318ba62a0e66b24ba2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b7a94d8e1ab4dc19fe39f67fd355ce9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8075a4f16ce40e38ff62c42126e0f35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e78df50b3d024f588da3466ab457dc98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93b769b0ee8546a5bbaca76de642fbe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1a9d58ffd054fbaa299d39dde4b114d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c4e2778cbd24b21b38ee37fc94ac805": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}